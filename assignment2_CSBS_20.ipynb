{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 5: Create a Python-based solution to calculate the\n",
        "semantic similarity between two sentences using techniques like\n",
        "cosine similarity or word embeddings, validating it with the input\n",
        "\"This is a sample sentence.\" and \"This sentence is just a sample.\"\n",
        "to produce a similarity score of 0.8."
      ],
      "metadata": {
        "id": "3XWnEVfKpooH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-Assignment 5\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def compute_similarity(sentence1, sentence2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "    return round(similarity_score, 2)  # Rounding for better readability\n",
        "\n",
        "input_sentence1 = \"This is a sample sentence.\"\n",
        "input_sentence2 = \"This sentence is just a sample.\"\n",
        "output = compute_similarity(input_sentence1, input_sentence2)\n",
        "print(output)  # Expected Output: 0.8 (approximately)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHXxE2LJBT7U",
        "outputId": "4bb8b872-c3be-4f54-bd0f-68fe22ba3f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 6: Develop and implement a Python-based solution\n",
        "to compute the Term Frequency-Inverse Document Frequency\n",
        "(TF-IDF) scores for a small dataset using libraries such as sklearn\n",
        ", validating it with the input {'text': ['This is a sample document.',\n",
        "\n",
        "'Another document with different content.']} to generate the TF-\n",
        "IDF matrix."
      ],
      "metadata": {
        "id": "eqTzwVFlpwAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-Assignment 6\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_tfidf(data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(data[\"text\"])\n",
        "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
        "\n",
        "# Test Case\n",
        "dataset = {\"text\": [\"This is a sample document.\", \"Another document with different content.\"]}\n",
        "tfidf_values, feature_names = compute_tfidf(dataset)\n",
        "\n",
        "# Printing results\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpp0nw0sBdBf",
        "outputId": "95513a6c-2910-443d-ef8e-8ab40fa35ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['another' 'content' 'different' 'document' 'is' 'sample' 'this' 'with']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.37997836 0.53404633 0.53404633\n",
            "  0.53404633 0.        ]\n",
            " [0.47107781 0.47107781 0.47107781 0.33517574 0.         0.\n",
            "  0.         0.47107781]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 7: Create a Python-based text preprocessing\n",
        "program that standardizes input by converting it to lowercase,\n",
        "removing punctuation, and eliminating stopwords using libraries\n",
        "such as nltk or re. Validate the program using the input \"This is a\n",
        "sample text. It contains punctuation and stopwords.\" to produce\n",
        "the output \"sample text contains\"."
      ],
      "metadata": {
        "id": "NHz4FHWRpycT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment 7: Text Preprocessing\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Input text\n",
        "text = \"This is a sample text. It contains punctuation and stopwords.\"\n",
        "\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Tokenize words\n",
        "words = text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "# Join processed words\n",
        "processed_text = ' '.join(filtered_words)\n",
        "\n",
        "print(processed_text)  # Expected Output: \"sample text contains\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4VsDv_rBf9m",
        "outputId": "65235dc5-fc1d-46ac-8e07-b330d88783c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text contains punctuation stopwords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 8: Develop and execute a Python-driven approach\n",
        "to identify named entities within a given sentence, leveraging a\n",
        "named entity recognition module (e.g., nltk's ne_chunk), and\n",
        "validate it using the input \"John Smith works at Google.\" to\n",
        "produce the output [(\"John Smith\", \"PERSON\"), (\"Google\",\n",
        "\"ORGANIZATION\")]."
      ],
      "metadata": {
        "id": "tcz4_yBup0jR"
      }
    },
    {
      "source": [
        "# Assignment 8: Text Preprocessing\n",
        "import nltk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "# Download the averaged_perceptron_tagger resource\n",
        "nltk.download(\"averaged_perceptron_tagger\")  # This line downloads the necessary data\n",
        "# Download the missing 'punkt_tab' data package and the averaged_perceptron_tagger_eng\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the missing resource\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n",
        "# Function to extract named entities\n",
        "def get_named_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "\n",
        "    named_entities = []\n",
        "\n",
        "    for chunk in chunked:\n",
        "        if isinstance(chunk, Tree):  # Check if it's a named entity\n",
        "            entity_name = \" \".join(token for token, pos in chunk.leaves())\n",
        "            entity_type = chunk.label()\n",
        "            named_entities.append((entity_name, entity_type))\n",
        "\n",
        "    return named_entities\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"John Smith works at Google.\"\n",
        "\n",
        "# Get named entities\n",
        "entities = get_named_entities(sentence)\n",
        "\n",
        "# Print the output\n",
        "print(entities)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxg7Xh-SDW_X",
        "outputId": "f642a3f2-394b-48f5-c24f-71c91b8e0045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'PERSON'), ('Smith', 'PERSON'), ('Google', 'ORGANIZATION')]\n"
          ]
        }
      ]
    }
  ]
}
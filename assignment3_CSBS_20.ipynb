{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3\n",
        "\n",
        "1.   Data Preprocessing.\n",
        "2.   Create a Skip-gram model using PyTorch/TensorFlow or NumPy.\n",
        "3.   Modify the Skip-gram model to CBOW.\n",
        "4.   Implement GloVe."
      ],
      "metadata": {
        "id": "E48qhMp6LIAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpmIM6bvK4cH",
        "outputId": "5cab52a2-22f7-40af-d2ad-66d64975059a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "# Download the missing 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # This line downloads the necessary data\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "import re\n",
        "\n",
        "# Load a sample corpus (Jane Austen's book)\n",
        "text = gutenberg.raw('austen-emma.txt')[:50000]\n",
        "\n",
        "# Tokenize and clean\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(text)\n",
        "print(tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wod1WVceLLiW",
        "outputId": "b902663b-fc5c-4f76-f7ec-c93ad6d439c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sj0rXqdLOgE",
        "outputId": "afede813-a372-44e3-acdd-08a1e2ca9aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    skip_grams = []\n",
        "    for i, target in enumerate(tokens):\n",
        "        context_window = tokens[max(i - window_size, 0): i] + tokens[i + 1: i + window_size + 1]\n",
        "        for context in context_window:\n",
        "            skip_grams.append((word_to_idx[target], word_to_idx[context]))\n",
        "    return skip_grams\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    cbow_data = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
        "        target = tokens[i]\n",
        "        cbow_data.append(([word_to_idx[word] for word in context], word_to_idx[target]))\n",
        "    return cbow_data\n",
        "\n",
        "skipgram_data = generate_skipgram_data(tokens, window_size)\n",
        "cbow_data = generate_cbow_data(tokens, window_size)\n",
        "\n",
        "print(\"Skip-gram Example:\", skipgram_data[:3])\n",
        "print(\"CBOW Example:\", cbow_data[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St5Ih-OaLQby",
        "outputId": "f98a1c90-c6c9-426f-8de0-c100113b7f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-gram Example: [(471, 207), (471, 836), (207, 471)]\n",
            "CBOW Example: [([471, 207, 122, 1657], 836), ([207, 836, 1657, 764], 122), ([836, 122, 764, 237], 1657)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, center_word):\n",
        "        embed = self.embeddings(center_word)\n",
        "        out = self.linear(embed)\n",
        "        return out\n",
        "\n",
        "model = SkipGram(vocab_size, embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for target, context in random.sample(skipgram_data, 1000):\n",
        "        input_tensor = torch.tensor([target], dtype=torch.long)\n",
        "        context_tensor = torch.tensor([context], dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_tensor)\n",
        "        loss = criterion(output, context_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"Skip-gram Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RqqbbYHLSzr",
        "outputId": "07158c0f-bcad-4172-a471-24612f513ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-gram Epoch: 0 Loss: 7288.345961332321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_words):\n",
        "        embeds = self.embeddings(context_words)\n",
        "        avg_embed = embeds.mean(dim=0).view(1, -1)\n",
        "        out = self.linear(avg_embed)\n",
        "        return out\n",
        "\n",
        "cbow_model = CBOW(vocab_size, embedding_dim)\n",
        "cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for context, target in random.sample(cbow_data, 1000):\n",
        "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
        "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
        "\n",
        "        cbow_optimizer.zero_grad()\n",
        "        output = cbow_model(context_tensor)\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "        cbow_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"CBOW Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9jbOAjjLWe8",
        "outputId": "24309e1c-fa3a-4603-afae-b76d986fcdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Epoch: 0 Loss: 7167.2594165802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, window_size):\n",
        "    matrix = defaultdict(lambda: defaultdict(int))\n",
        "    for idx, word in enumerate(tokens):\n",
        "        for j in range(max(idx - window_size, 0), min(idx + window_size + 1, len(tokens))):\n",
        "            if idx == j:\n",
        "                continue\n",
        "            matrix[word][tokens[j]] += 1\n",
        "    return matrix\n",
        "\n",
        "co_matrix = build_cooccurrence_matrix(tokens, window_size)\n",
        "\n",
        "X = np.zeros((vocab_size, vocab_size))\n",
        "for w1 in co_matrix:\n",
        "    for w2 in co_matrix[w1]:\n",
        "        i = word_to_idx[w1]\n",
        "        j = word_to_idx[w2]\n",
        "        X[i][j] = co_matrix[w1][w2]\n",
        "\n",
        "embedding_dim = 50\n",
        "W = np.random.rand(vocab_size, embedding_dim)\n",
        "W_context = np.random.rand(vocab_size, embedding_dim)\n",
        "bias = np.random.rand(vocab_size)\n",
        "bias_context = np.random.rand(vocab_size)\n",
        "\n",
        "def glove_loss(i, j, xij):\n",
        "    weight = (xij / 100) ** 0.75 if xij < 100 else 1\n",
        "    dot = np.dot(W[i], W_context[j]) + bias[i] + bias_context[j]\n",
        "    return weight * ((dot - np.log(xij)) ** 2)\n",
        "\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(vocab_size):\n",
        "            if X[i][j] > 0:\n",
        "                total_loss += glove_loss(i, j, X[i][j])\n",
        "    print(\"GloVe Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AuQVFtDLZXn",
        "outputId": "8505806c-0576-4f0f-f9a3-759dc35105d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe Epoch: 0 Loss: 166903.54231195807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(word, embeddings, top_n=5):\n",
        "    idx = word_to_idx[word]\n",
        "    vec = embeddings[idx]\n",
        "    sims = np.dot(embeddings, vec)\n",
        "    sorted_idx = np.argsort(-sims)\n",
        "    return [idx_to_word[i] for i in sorted_idx[1:top_n+1]]\n",
        "\n",
        "print(\"Similar to 'emma' in Skip-gram:\", get_similar('emma', model.embeddings.weight.detach().numpy()))\n",
        "print(\"Similar to 'emma' in CBOW:\", get_similar('emma', cbow_model.embeddings.weight.detach().numpy()))\n",
        "print(\"Similar to 'emma' in GloVe:\", get_similar('emma', W))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc_uetJfLbxA",
        "outputId": "1c5d0a09-9e7d-4c26-bf19-4f8e13635be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar to 'emma' in Skip-gram: ['dissuade', 'included', 'widowerfather', 'ill', 'quite']\n",
            "Similar to 'emma' in CBOW: ['sit', 'congratulation', 'an', 'mr', 'over']\n",
            "Similar to 'emma' in GloVe: ['soft', 'widowerfather', 'emma', 'afford', 'prodigies']\n"
          ]
        }
      ]
    }
  ]
}